{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Playing with embeddings.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A7LkGfi9QpjU",
        "colab_type": "text"
      },
      "source": [
        "# Playing with embeddings\n",
        "\n",
        "This article covers the most used text preprocessing technique used to convert text into numbers, so the text sentences can be feed to Machine Learning (ML) and Deep Learning (DL) models.\n",
        "\n",
        "Intelligent models don't take as input raw text, they only work with numeric data. *Vectorizing* text is the process of transforming text into numeric tensors.\n",
        "\n",
        "There are different ways to vectorize raw text. The top three popular vectorizing methods are:\n",
        "\n",
        "- Segment text into words, and transform each word into a vector.\n",
        "- Segment text into characters, and transform each character into a vector.\n",
        "- Extract n-grams of words or characters, and transform each n-gram into a vector.\n",
        " *N-grams* are overlapping groups of multiple consecutive words or characters. [[1]](#references)\n",
        "\n",
        "The different units which a sentence is split into, are called *tokens*. \n",
        "\n",
        "> Note: All *vectorizing* methods consist of first *tokenizing* the sentence (Split a sentence into different units) and then associating a vector to each *token*.\n",
        "\n",
        "First, we will introduce *one-hot encoding (OHE)* and then we are going to dive into *word embeddings*, which is the most used word-level *vectorization* mehtod among the DL community.\n",
        "\n",
        "> Note: Some advanced methods, such as pretrained Transformers (BERT[[2]](#references), RoBERTa[[3]](#references), ...) use *n-grams* level tokenization. That is because n-grams tokenization can efficiently handle with huge vocabularies.\n",
        "\n",
        "![Vectorization](https://freecontent.manning.com/wp-content/uploads/Chollet_DLfT_01.png)\n",
        "\n",
        "## One-hot Encoding (OHE)\n",
        "\n",
        "OHE is the most basic way to convert a token into a vector. It consists of associating a unique integer index with every word and  then  turning  this  integer  index  *i*  into  a  binary  vector  of  size  N (the size of the vocabulary); the vector is all zeros except for the *i*th entry, which is 1. [[1]](#references)\n",
        "\n",
        "We can use scikit-learn(sklearn) to convert tokens into vectors using OHE. To do so, sklearn provides a OneHotEncoder class, which like all the other sklearn classes, has a `fit` and `transform` method. These methods do the following tasks:\n",
        "\n",
        "- `fit`: Learns the vocabulary of our dataset, and learns how to associate a token with an integer index.\n",
        "- `transform`: Accordingly with the understanding acquired in the `fit` method transforms each token to its associated vector."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4FdzItAjQzNw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "# Import ohe class\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "# Declare text examples\n",
        "texts = ['I study Computer Science at university',\n",
        "         'My dog is called Peludet']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rFY_ihnakGwV",
        "colab_type": "text"
      },
      "source": [
        "As we explained on the introduction first we need to tokenize the texts. We will be working with a word-level tokenization, so we are going to split sentences by `' '` character."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e0WKNOD1kh-K",
        "colab_type": "code",
        "outputId": "720bc944-ac93-468b-ab6b-f04f62dbe3c7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "# Split texts by ' '\n",
        "tokenized_texts = [text.split() for text in texts]\n",
        "tokenized_texts"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['I', 'study', 'Computer', 'Science', 'at', 'university'],\n",
              " ['My', 'dog', 'is', 'called', 'Peludet']]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OIlFrXJhkuWg",
        "colab_type": "code",
        "outputId": "d17bb3f7-7bea-4f54-aecb-74e2bae9d156",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 223
        }
      },
      "source": [
        "# Flat the texts list so the tokens are ina single list\n",
        "tokenized_texts = [[t] for tokens in tokenized_texts for t in tokens]\n",
        "tokenized_texts"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['I'],\n",
              " ['study'],\n",
              " ['Computer'],\n",
              " ['Science'],\n",
              " ['at'],\n",
              " ['university'],\n",
              " ['My'],\n",
              " ['dog'],\n",
              " ['is'],\n",
              " ['called'],\n",
              " ['Peludet']]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aQrrm_zxlXvd",
        "colab_type": "text"
      },
      "source": [
        "Now we are ready to feed the tokens to sklearn `OneHotEncoder` class `fit` method.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F2_731EVlk8s",
        "colab_type": "code",
        "outputId": "22651db5-db60-4002-e7dd-fb70b27a92b8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        }
      },
      "source": [
        "# Create OneHotEncoder instance\n",
        "ohe = OneHotEncoder()\n",
        "\n",
        "# Feed the tokens to fit method\n",
        "ohe.fit(tokenized_texts)\n",
        "\n",
        "# See what out OneHotEncoder has learned\n",
        "print('Vocabulary:', ohe.get_feature_names())\n",
        "print('Vocabulary len:', len(ohe.get_feature_names()))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vocabulary: ['x0_Computer' 'x0_I' 'x0_My' 'x0_Peludet' 'x0_Science' 'x0_at'\n",
            " 'x0_called' 'x0_dog' 'x0_is' 'x0_study' 'x0_university']\n",
            "Vocabulary len: 11\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hvQ2f0JHm1tx",
        "colab_type": "code",
        "outputId": "6abd414e-7ebb-4c03-e9cf-1157a147dfb1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "texts[0].split()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['I', 'study', 'Computer', 'Science', 'at', 'university']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NyQQvVRrmhlQ",
        "colab_type": "code",
        "outputId": "1aa5ee0e-892c-4a6f-abbb-6c7421967fe8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        }
      },
      "source": [
        "# Now we can covert out tokens to vectors\n",
        "## For a more clear example we will only vectorize first sentence\n",
        "encoded_sentence = ohe.transform(np.array(texts[0].split()).reshape(-1, 1))\n",
        "for t, v in zip(texts[0].split(), encoded_sentence):\n",
        "  print('Token: {} -> Vector: {}'.format(t, v.toarray()))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Token: I -> Vector: [[0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "Token: study -> Vector: [[0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]]\n",
            "Token: Computer -> Vector: [[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "Token: Science -> Vector: [[0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]]\n",
            "Token: at -> Vector: [[0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]]\n",
            "Token: university -> Vector: [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QOrBapHUot5U",
        "colab_type": "text"
      },
      "source": [
        "As we can see each token is associated with an integer value *i*, which corresponds to the array *i*th position which at time is set to 1.\n",
        "\n",
        "![One Hot Encoding](https://raw.githubusercontent.com/tensorflow/docs/r2.0rc/site/en/r2/tutorials/text/images/one-hot.png)\n",
        "\n",
        "**Even though OHE is simple, it is inefficient**. A one-hot encoded vector is sparse (meaning, most indicices are zero). Imagine we have 10,000 words in the vocabulary. To one-hot encode each word, we would create a vector where 99.99% of the elements are zero.\n",
        "\n",
        "Now imagine that we are trying to vectorize these three words:\n",
        "\n",
        "- great\n",
        "- good\n",
        "- bad\n",
        "\n",
        "If we vectorize them using OHE we will get this result:\n",
        "\n",
        "```python\n",
        "assert good  == [1, 0, 0]\n",
        "assert great == [0, 1, 0]\n",
        "assert bad   == [0, 0, 1]\n",
        "```\n",
        "\n",
        "In this case, OHE vectorization result is telling us that `good` is as different as `great` and `bad`. So we need another vectorization method that can handle semantic similarities, here is where **Word Embeddings** is important."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZsBmgXqIY2Al",
        "colab_type": "text"
      },
      "source": [
        "## References\n <a id="references"></a>",
        "\n",
        "- [1] Deep Learning with Python - François Chollet <a id=\"ref-1\"></a>\n",
        "- [2] BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding - https://arxiv.org/abs/1810.04805\n",
        "\n",
        "- [3] RoBERTa: An optimized method for pretraining self-supervised NLP systems - https://arxiv.org/abs/1907.11692"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o1Dz7Uu3Ru7T",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    }
  ]
}
